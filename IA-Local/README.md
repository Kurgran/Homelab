#  Stack IA Local - Homelab

> Projet de formation en cybersÃ©curitÃ© - DÃ©ploiement d'une solution d'intelligence artificielle 100% locale et souveraine dans un environnement homelab, dÃ©montrant la maÃ®trise de l'infrastructure IA, Docker, et des principes de souverainetÃ© des donnÃ©es.

## ðŸ“‹ Table des matiÃ¨res

- [Contexte et justification](#-contexte-et-justification-du-projet)
- [Architecture technique](#-architecture-technique)
- [Installation](#-installation)
- [Configuration RAG](#-configuration-rag-avec-chromadb)
- [Utilisation](#-utilisation)
- [Cas d'usage](#-cas-dusage)
- [Troubleshooting](#-troubleshooting)
- [Performances](#-performances)
- [SÃ©curitÃ© & ConformitÃ©](#-sÃ©curitÃ©--conformitÃ©)
- [Ã‰volutions futures](#-Ã©volutions-futures)

---

## ðŸŽ¯ Contexte et justification du projet

### Pourquoi une IA en local ?

Ce projet explore le dÃ©ploiement d'un modÃ¨le de langage (LLM) en local dans un homelab.
L'idÃ©e est de pouvoir interroger un modÃ¨le d'IA sur des documents personnels ou non

**ProblÃ©matiques adressÃ©es :**
- **SouverainetÃ© des donnÃ©es** : Aucune donnÃ©e envoyÃ©e Ã  l'Ã©tranger 
- **ConformitÃ© RGPD** : 100% on-premise, audit trail complet, droit d'effacement
- **CoÃ»t maÃ®trisÃ©** : Pas d'abonnement cloud, infrastructure existante valorisÃ©e
- **Performance** : Latence minimale (local), pas de dÃ©pendance internet
- **SÃ©curitÃ©** : ContrÃ´le total de la stack, logs internes, isolation rÃ©seau

### Objectif de ce projet

DÃ©montrer la capacitÃ© Ã  dÃ©ployer et sÃ©curiser une **stack IA complÃ¨te** (type ChatGPT) dans un environnement homelab professionnel, en combinant :
- **CompÃ©tences Data/BI** : RAG, embeddings, prÃ©paration datasets
- **CompÃ©tences Cyber** : SÃ©curisation infra, audit
- **CompÃ©tences Infrastructure** : Docker, monitoring, architecture hybride


### Technologies utilisÃ©es

- **Mac M4 Pro** 
- **Docker Compose** - Orchestration conteneurs
- **Ollama** - Runtime LLM local avec optimisations Apple Silicon
- **OpenWebUI** - Interface ChatGPT-like, authentification multi-users
- **ChromaDB** - Base vectorielle pour RAG (Retrieval Augmented Generation)
- **Python 3.x** - Scripts d'automatisation et ingestion RAG

---

## ðŸ—ï¸ Architecture technique

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Utilisateur (Navigateur Web)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼ http://192.168.1.50:3001
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          OpenWebUI (Port 3001)               â”‚
â”‚  - Interface ChatGPT-like                    â”‚
â”‚  - Authentification                          â”‚
â”‚  - Gestion conversations                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           â”‚              â”‚
           â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB        â”‚    â”‚     Ollama       â”‚
â”‚  (Port 8000)      â”‚    â”‚  (Port 11434)    â”‚
â”‚  Recherche        â”‚    â”‚  GÃ©nÃ©ration      â”‚
â”‚  sÃ©mantique       â”‚    â”‚  texte LLM       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Infrastructure

**Plateforme** : Mac M4 Pro 

**Ressources allouÃ©es** :
- Docker Desktop 
- Ollama : AccÃ¨s GPU via Metal
- ChromaDB : 2 Go RAM
- OpenWebUI : 1 Go RAM

**RÃ©seau** :
- IP statique homelab : `192.168.1.50` (exemple)
- Ports exposÃ©s : 
  - `3001` : OpenWebUI (interface web)
  - `11434` : Ollama API (compatible OpenAI)
  - `8000` : ChromaDB (localhost uniquement)

**Stockage** :
- Volumes Docker persistants
- ModÃ¨les LLM : ~20-50 Go selon modÃ¨les
- ChromaDB embeddings : ~100 Mo - 5 Go selon corpus
- Backups : NAS Synology (rsync quotidien recommandÃ©)

---

## ðŸš€ Installation

### PrÃ©requis

**Versions testÃ©es :**
- Docker Desktop : 4.26+ (pour Mac Apple Silicon)
- Ollama : latest (image officielle)
- OpenWebUI : main branch
- ChromaDB : latest
- macOS : Sonoma 14.0+ ou Sequoia 15.0+

**Installation Docker Desktop :**

```bash
# Via Homebrew (recommandÃ©)
brew install --cask docker

# VÃ©rification
docker --version
docker-compose --version
```

### Ã‰tape 1 : PrÃ©paration de l'environnement

```bash
# CrÃ©er le dossier projet
mkdir -p ~/ai-sovereign-stack
cd ~/ai-sovereign-stack

# CrÃ©er les sous-dossiers
mkdir -p documents scripts
```

### Ã‰tape 2 : DÃ©ploiement de la stack

**CrÃ©er le fichier `docker-compose.yml`** (voir fichier dans ce repo)

```bash
# Lancer tous les services
docker-compose up -d

# VÃ©rifier que tout tourne
docker-compose ps

# Consulter les logs
docker-compose logs -f
```

**Sortie attendue :**
```
[+] Running 4/4
 âœ” Network ai-sovereign-stack_default    Running
 âœ” Container ollama                      Running
 âœ” Container chromadb                    Started
 âœ” Container openwebui                   Running
```

### Ã‰tape 3 : TÃ©lÃ©chargement du premier modÃ¨le

```bash
# TÃ©lÃ©charger Mistral 7B (modÃ¨le franÃ§ais optimisÃ©)
docker exec ollama ollama pull mistral

# VÃ©rifier les modÃ¨les disponibles
docker exec ollama ollama list
```

**DurÃ©e** : 5-10 minutes selon connexion internet (~4 Go Ã  tÃ©lÃ©charger)

### Ã‰tape 4 : Configuration initiale OpenWebUI

1. Ouvrir navigateur : `http://localhost:3001` (ou IP Mac)
2. **Premier accÃ¨s** : CrÃ©er compte admin
   - Email : votre email
   - Username : votre username
   - Password : mot de passe sÃ©curisÃ© (min 8 caractÃ¨res)
3. **Connexion automatique** aprÃ¨s crÃ©ation
4. **SÃ©lectionner modÃ¨le** : `mistral:latest` dans le menu dÃ©roulant

**Test rapide** :
```
Prompt : "Bonjour ! Explique-moi en 2 phrases ce qu'est l'IA souveraine."
```

âœ… Si la rÃ©ponse arrive en franÃ§ais et est cohÃ©rente â†’ Installation rÃ©ussie !

---

## ðŸ”§ Configuration RAG avec ChromaDB

### Connexion OpenWebUI â†’ ChromaDB

**Dans OpenWebUI** :
1. Cliquer sur avatar/nom (haut droite)
2. **Admin Panel** > **Documents**
3. **Add Vector Database** :
   - Type : `ChromaDB`
   - URL : `http://chromadb:8000`
   - Collection : `homelab_docs` (sera crÃ©Ã©e automatiquement)
4. **Save**

âš ï¸ **Important** : Utiliser `http://chromadb:8000` (nom du service Docker), pas `localhost:8000`

### Script d'ingestion automatique

**Installation des dÃ©pendances Python** :

```bash
# CrÃ©er environnement virtuel
cd ~/ai-sovereign-stack
python3 -m venv venv
source venv/bin/activate

# Installer dÃ©pendances
pip install -r scripts/requirements.txt
```

**CrÃ©er des documents test** :

```bash
# Exemple : Documentation ports homelab
cat > documents/homelab-ports.txt << 'EOF'
# Configuration Ports Homelab

## Projet monitoring-stack
- Grafana : port 3000 (dashboards visualisation)
- Prometheus : port 9090 (collecte mÃ©triques)
- Node Exporter : port 9100 (mÃ©triques systÃ¨me)

## Projet ai-sovereign-stack
- Ollama API : port 11434 (API LLM local)
- OpenWebUI : port 3001 (interface ChatGPT-like)
- ChromaDB : port 8000 (base vectorielle)

## Infrastructure
- pfSense : 192.168.1.1 (routeur/firewall)
- NAS Synology : 192.168.1.10 (stockage)
- NUC Proxmox : 192.168.1.20 (virtualisation)
EOF
```

**Lancer l'ingestion** :

```bash
cd ~/ai-sovereign-stack
source venv/bin/activate
python3 scripts/ingest_docs.py
```

**Sortie attendue :**
```
============================================================
ðŸš€ INGESTION DOCUMENTS - RAG ChromaDB
============================================================

ðŸ“¡ Connexion Ã  ChromaDB...
âœ… Connexion ChromaDB OK

ðŸ“š Gestion collection 'homelab_docs'...
âœ… Collection 'homelab_docs' prÃªte

ðŸ“„ Traitement : homelab-ports.txt
   âœ‚ï¸  DÃ©coupÃ© en 3 morceaux
   âœ… IndexÃ© : 3 morceaux

============================================================
ðŸ“Š RÃ‰SUMÃ‰ DE L'INGESTION
============================================================
âœ… Documents traitÃ©s : 1
âœ… Morceaux indexÃ©s : 3
```

---

## ðŸ“Š Utilisation

### AccÃ¨s aux services

| Service | URL | Credentials |
|---------|-----|-------------|
| **Interface OpenWebUI** | `http://192.168.1.50:3001` | DÃ©finis lors du premier accÃ¨s |
| **API Ollama** | `http://192.168.1.50:11434` | Aucun (accÃ¨s local) |
| **API ChromaDB** | `http://localhost:8000` | Aucun (localhost uniquement) |

### Commandes utiles

```bash
# VÃ©rifier le statut de tous les services
docker-compose ps

# Consulter les logs en temps rÃ©el
docker-compose logs -f

# RedÃ©marrer un service
docker-compose restart ollama

# ArrÃªter la stack complÃ¨te
docker-compose down

# Lister les modÃ¨les Ollama disponibles
docker exec ollama ollama list

# Supprimer un modÃ¨le
docker exec ollama ollama rm [nom_modele]
```

---

## ðŸŽ¯ Cas d'usage

### Cas d'usage 1 : Conversation simple

1. AccÃ©der Ã  OpenWebUI : `http://192.168.1.50:3001`
2. SÃ©lectionner modÃ¨le : `mistral:latest`
3. Poser question dans l'interface

### Cas d'usage 2 : RAG - Questions sur vos documents

**Activer RAG dans OpenWebUI** :
1. Toggle "Use Documents" ou icÃ´ne ðŸ“š
2. SÃ©lectionner collection : `homelab_docs`
3. Poser question sur vos documents indexÃ©s

**Exemple** :
```
Prompt : "Quels sont les ports utilisÃ©s dans mon homelab ?"

RÃ©ponse (avec RAG) :
"Selon ta documentation homelab, voici les ports utilisÃ©s :
- Grafana : port 3000 (monitoring)
- Prometheus : port 9090 (mÃ©triques)
- Ollama API : port 11434 (IA locale)
- OpenWebUI : port 3001 (interface web)
[...]"
```

**Sans RAG**, l'IA donnerait une rÃ©ponse gÃ©nÃ©rique sur les ports courants en homelab, pas TES ports spÃ©cifiques.

### Cas d'usage 3 : Utilisation de l'API Ollama

```bash
# GÃ©nÃ©ration de texte via API
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Explique la souverainetÃ© numÃ©rique en 2 phrases",
  "stream": false
}'

# Chat conversation
curl http://localhost:11434/api/chat -d '{
  "model": "mistral",
  "messages": [
    {"role": "user", "content": "Bonjour!"}
  ]
}'
```

---

## ðŸ›  Troubleshooting

### Erreur : "Connection refused" sur port 11434

**Cause** : Container Ollama pas dÃ©marrÃ© ou pas prÃªt

**Solution** :
```bash
# VÃ©rifier statut
docker ps | grep ollama

# Si absent, redÃ©marrer
docker-compose restart ollama

# Consulter logs
docker logs ollama
```

### Erreur : OpenWebUI ne se connecte pas Ã  Ollama

**Cause** : Variable `OLLAMA_BASE_URL` incorrecte

**Solution** :
```bash
# VÃ©rifier configuration
docker exec openwebui env | grep OLLAMA

# Doit afficher : OLLAMA_BASE_URL=http://ollama:11434
```

### Erreur : ChromaDB "Collection not found"

**Cause** : Collection pas encore crÃ©Ã©e

**Solution** :
```bash
# VÃ©rifier collections existantes
python3 -c "
import chromadb
client = chromadb.HttpClient(host='localhost', port=8000)
print(client.list_collections())
"

# Si vide, lancer ingestion
python3 scripts/ingest_docs.py
```

### Performance : GÃ©nÃ©ration de texte lente

**Cause** : ModÃ¨le trop gros pour la RAM disponible

**Solution** :
```bash
# VÃ©rifier usage mÃ©moire
docker stats

# Passer Ã  un modÃ¨le plus lÃ©ger si nÃ©cessaire
docker exec ollama ollama pull mistral  # 7B au lieu de 13B+
```

---

## ðŸ“ˆ Performances

### Metrics observÃ©es (Mac M4 Pro, 64 Go RAM)

| ModÃ¨le | Taille | Tokens/sec | Latence premiÃ¨re rÃ©ponse | RAM utilisÃ©e |
|--------|--------|------------|--------------------------|--------------|
| Mistral 7B | 4.1 GB | ~40 tok/s | <100 ms | ~8 GB |
| Llama 3.2 8B | 4.7 GB | ~35 tok/s | <150 ms | ~9 GB |
| Qwen 2.5 Coder 32B | 19 GB | ~15 tok/s | <300 ms | ~24 GB |

**Consommation Ã©nergÃ©tique** : ~50W en charge (vs 150-200W serveur x86 classique)

**Benchmark RAG (ChromaDB)** :
- Recherche sÃ©mantique : <100 ms (corpus 1000 documents)
- Indexation : ~2-5 sec par document (selon taille)

---

## ðŸ”’ SÃ©curitÃ© & ConformitÃ©

### SouverainetÃ© des donnÃ©es

- âœ… **100% on-premise** : Aucune donnÃ©e envoyÃ©e Ã  OpenAI, Google ou autre cloud US
- âœ… **RGPD by design** : 
  - DonnÃ©es hÃ©bergÃ©es localement
  - Droit d'effacement : `docker-compose down -v`
  - Chiffrement au repos : Volumes Docker chiffrÃ©s (FileVault macOS)
  
### Authentification

- OpenWebUI : Authentification par email/password
- API Ollama : AccÃ¨s localhost uniquement (ou via reverse proxy avec auth)
- ChromaDB : AccÃ¨s restreint au rÃ©seau Docker

### Backups recommandÃ©s

```bash
#!/bin/bash
# Script backup automatique (Ã  planifier via cron)
BACKUP_DIR="/Volumes/NAS/backups/ai-stack"
DATE=$(date +%Y%m%d-%H%M%S)

# ArrÃªter services
docker-compose -f ~/ai-sovereign-stack/docker-compose.yml down

# Backup volumes
sudo rsync -avz /var/lib/docker/volumes/ $BACKUP_DIR/volumes-$DATE/

# Backup configuration
tar -czf $BACKUP_DIR/config-$DATE.tar.gz ~/ai-sovereign-stack/

# RedÃ©marrer services
docker-compose -f ~/ai-sovereign-stack/docker-compose.yml up -d
```

### Recommandations production

- Reverse proxy HTTPS (Nginx/Traefik) pour OpenWebUI
- Segmentation rÃ©seau VLAN
- Monitoring Prometheus + alertes
- Rate limiting API Ollama

---

## IdÃ©es d'Ã©volutions futures

**Phase 3 - Monitoring & SÃ©curitÃ©** (prÃ©vu) :
- [ ] IntÃ©gration Prometheus + Node Exporter
- [ ] Dashboards Grafana personnalisÃ©s
- [ ] Logs centralisÃ©s vers Wazuh SIEM
- [ ] Alerting automatique (service down, RAM saturÃ©e)

**Phase 4 - Cas d'usage avancÃ©s** :
- [ ] Chatbot support IT basÃ© sur documentation interne
- [ ] Analyse automatique logs pfSense avec IA
- [ ] GÃ©nÃ©ration rapports conformitÃ© ISO 27001
- [ ] Assistant troubleshooting avec base de connaissances

**Phase 5 - ScalabilitÃ©** :
- [ ] Load balancing Ollama (plusieurs instances)
- [ ] Migration vers Kubernetes (K3s)
- [ ] Haute disponibilitÃ© ChromaDB

---

## ðŸ“š Ressources

**Documentation officielle :**
- [Ollama](https://ollama.ai/docs)
- [OpenWebUI](https://docs.openwebui.com)
- [ChromaDB](https://docs.trychroma.com)

**CommunautÃ© :**
- [Discord Ollama](https://discord.gg/ollama)
- [Reddit r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)
- [Reddit r/selfhosted](https://reddit.com/r/selfhosted)




---

##  Conclusion


- DÃ©ployer une infrastructure IA complÃ¨te et sÃ©curisÃ©e
- MaÃ®triser l'architecture conteneurisÃ©e (Docker)
- Comprendre les enjeux de conformitÃ© 
- Automatiser les processus critiques
- Documenter de maniÃ¨re professionnelle

La mise en place de cette stack IA souveraine s'inscrit dans une dÃ©marche de **reconversion professionnelle en cybersÃ©curitÃ©**, dÃ©montrant un positionnement unique combinant expertise data, infrastructure et sÃ©curitÃ©.

---


*DerniÃ¨re mise Ã  jour : Janvier 2026*  
*Version : 1.0*  
*Projet rÃ©alisÃ© dans le cadre d'une reconversion en cybersÃ©curitÃ©*
